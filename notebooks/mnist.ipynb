{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Imports\n"
      ],
      "metadata": {
        "id": "LGaJxAq9jEOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lpT-tFRzCMvi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the neural network, load data, train it"
      ],
      "metadata": {
        "id": "AeAuE1TzjAtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a simple neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set the device to use for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set up the network and optimizer\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Load the training data\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                #       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])\n",
        "    ),\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(10):  # 10 epochs\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = nn.CrossEntropyLoss()(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yoe-IsTCQW7",
        "outputId": "96b0bcf9-1301-4302-b464-9985809856fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 91939963.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 66962793.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28918513.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5658012.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.317573\n",
            "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.657261\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.776416\n",
            "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.348041\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.373221\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.205080\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.244896\n",
            "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.279838\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.231343\n",
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.244903\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.371983\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.112842\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.228686\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.279742\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.254930\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.180936\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.116016\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.142357\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.106645\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.205759\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.223503\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.168062\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.241345\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.304694\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.136150\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.156959\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.081209\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.048276\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.140245\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.100335\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.120944\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.254992\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.137634\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.088806\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.166088\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.076424\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.062228\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.060589\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.067779\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.060288\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.109036\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.046837\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.147028\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.133354\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.106340\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.046092\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.047312\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.095545\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.052653\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.227938\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.060689\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.158247\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.075184\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.170296\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.034723\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.157791\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.101073\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.046185\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.152971\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.072451\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.043512\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.047127\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.120809\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.020112\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.108626\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.086851\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.035645\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.053970\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.037062\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.023468\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.040120\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.075632\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.057603\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.037359\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.062695\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.032579\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.324037\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.069610\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.100225\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.020812\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.054470\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.041672\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.032288\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.013789\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.025988\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.123977\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.037354\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.085450\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.107739\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.029786\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.143612\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.013581\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.076537\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.069755\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.112955\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.027348\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.044506\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.019717\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.082381\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.075307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                   #    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])\n",
        "    ),\n",
        "    batch_size=1000, shuffle=True)\n"
      ],
      "metadata": {
        "id": "pKORePvqIQqj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "q_model_dict = deepcopy(model.state_dict())\n"
      ],
      "metadata": {
        "id": "CNpWojE40ngq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct and train an Observer Model.\n",
        "\n",
        "This network records the maximum and minimum of each layer in the training set when loaded with the trained weights from the previous step"
      ],
      "metadata": {
        "id": "qniMb94VjMnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ObserveNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ObserveNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        self.so_1_max = torch.tensor(-float('inf'))\n",
        "        self.so_2_max = torch.tensor(-float('inf'))\n",
        "        self.so_1_min = torch.tensor(float('inf'))\n",
        "        self.so_2_min = torch.tensor(float('inf'))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 784)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        temp_max = torch.max(x)\n",
        "        temp_min = torch.min(x)\n",
        "        self.so_1_max = temp_max if temp_max > self.so_1_max else self.so_1_max\n",
        "        self.so_1_min = temp_min if temp_min < self.so_1_min else self.so_1_min\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        temp_max = torch.max(x)\n",
        "        temp_min = torch.min(x)\n",
        "        self.so_2_max = temp_max if temp_max > self.so_2_max else self.so_2_max\n",
        "        self.so_2_min = temp_min if temp_min < self.so_2_min else self.so_2_min\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "o_net = ObserveNet()\n",
        "o_net.load_state_dict(q_model_dict)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()  # set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():  # disable gradient computation\n",
        "        for data, target in test_loader:\n",
        "            data = torch.round(data)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += nn.CrossEntropyLoss()(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "test(o_net, device, train_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf9F7HCLg54g",
        "outputId": "10f4f2e3-9c81-4ffb-a9fd-0a5604d740bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0008, Accuracy: 59186/60000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate scale constants of output for layer1 and layer2"
      ],
      "metadata": {
        "id": "Q9Bgm74tjcF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "so_1 = max(o_net.so_1_max, torch.abs(o_net.so_1_min)).float().item() / 127\n",
        "so_2 = max(o_net.so_2_max, torch.abs(o_net.so_2_min)).float().item() / 127\n",
        "print(so_1)\n",
        "print(so_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZN2ZHzvhm0X",
        "outputId": "83d70394-acd6-4340-8cb7-8a5a5e604282"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05031989315363366\n",
            "0.2053285433551458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize the layer weights"
      ],
      "metadata": {
        "id": "tZ87k5mRjk1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "q_model_dict = deepcopy(model.state_dict())\n",
        "\n",
        "# Returns the maximum value of a tensor devided by `m` which is the maximum n-bit int value\n",
        "# in quantization range.\n",
        "def max_scale(x: torch.tensor, m: int):\n",
        "  return torch.max(torch.abs(x)).item()/m\n",
        "\n",
        "# Scale of fc1 and fc2 determined by maximum value of int8 (127) and maximum weight value.\n",
        "def quantize_fc(x: torch.tensor, m: int):\n",
        "  return max_scale(x,m), torch.round(x / max_scale(x, m)).to(dtype=torch.int32)\n",
        "\n",
        "# Scale of bias determined by scale of the output of fc layer\n",
        "# Which is the scale of input multiplied by scale of the fc layer.\n",
        "def quantize_bias(x: torch.tensor, s: float):\n",
        "  # saturate\n",
        "  return torch.clip(torch.round(x/s), min=-127, max=127)\n",
        "\n",
        "s_fc1, q_model_dict['fc1.weight'] = quantize_fc(q_model_dict['fc1.weight'], 127)\n",
        "q_model_dict['fc1.bias'] = quantize_bias(q_model_dict['fc1.bias'], s_fc1 * 1 / 127)\n",
        "s_fc2, q_model_dict['fc2.weight'] = quantize_fc(q_model_dict['fc2.weight'], 127)\n",
        "q_model_dict['fc2.bias'] = quantize_bias(q_model_dict['fc2.bias'], s_fc2*so_1)\n",
        "\n",
        "class QuantNet(nn.Module):\n",
        "    def __init__(self, s_fc1, s_fc2, so_1, so_2):\n",
        "        super(QuantNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        self.s_fc1 = s_fc1\n",
        "        self.s_fc2 = s_fc2\n",
        "        self.so_1 = so_1\n",
        "        self.so_2 = so_2\n",
        "        self.s_x = 1 / 127\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "\n",
        "        # Scale input\n",
        "        x = torch.round(x / self.s_x)\n",
        "\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        # Requantize and saturation cast\n",
        "        x = np.clip(torch.round(x * ((self.s_fc1 * self.s_x) / self.so_1)), -127, 127)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        x = np.clip(torch.round(x * ((self.s_fc2 * self.so_1) / self.so_2)), -127, 127)\n",
        "\n",
        "        return x * self.so_2\n",
        "\n",
        "q_net = QuantNet(s_fc1=s_fc1, s_fc2=s_fc2, so_1=so_1, so_2=so_2)\n",
        "q_net.load_state_dict(q_model_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsC8AvHLldko",
        "outputId": "5d45b77c-e343-4da5-a575-d5746399a2de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print the scaling values so they can be used inside of Urbit"
      ],
      "metadata": {
        "id": "0GLzj2kilCSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(so_1)\n",
        "print(so_2)\n",
        "print(s_fc1)\n",
        "print(s_fc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyzedMfi9u02",
        "outputId": "68779f1a-9bb2-4c37-e565-6f423cee5459"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05031989315363366\n",
            "0.2053285433551458\n",
            "0.002196800051711676\n",
            "0.005814554184440553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the QuantNet on the test set"
      ],
      "metadata": {
        "id": "s1fBABBmktOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()  # set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():  # disable gradient computation\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += nn.CrossEntropyLoss()(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "# Run the test function\n",
        "test(q_net, device, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S0Q85t0C266",
        "outputId": "99a3180b-3d12-4377-baf5-bb44bc1db5ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9773/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write the QuantNet weights to disk as int32"
      ],
      "metadata": {
        "id": "mSS5OZSDkwyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in q_net.named_parameters():\n",
        "    print(param.detach().int().numpy())\n",
        "    def to_byte_array(array, name):\n",
        "        # Ensure the array is float32\n",
        "        array = array.astype(np.int32)\n",
        "\n",
        "        # Flatten the array in column-major order\n",
        "        flattened = array.flatten(order='C')\n",
        "\n",
        "        # Convert to byte array\n",
        "        byte_array = flattened.tobytes()\n",
        "\n",
        "        # Write byte array to a file\n",
        "        with open(name, 'wb') as f:\n",
        "          f.write(byte_array)\n",
        "\n",
        "\n",
        "        return byte_array\n",
        "\n",
        "    # Test the function\n",
        "    to_byte_array(param.detach().int().numpy(), f'{name}.mnist')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em2AFkF2FU1m",
        "outputId": "db687e28-2892-46ef-dcb4-fa280c2c2f2a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  8  -5  -3 ...   0   9  12]\n",
            " [ 14   3 -14 ...   0 -11 -12]\n",
            " [ 10 -12 -10 ...   7   4 -11]\n",
            " ...\n",
            " [ -9   5  15 ...  -8   5   3]\n",
            " [ 10  -8 -12 ...   2   6 -12]\n",
            " [  1 -10   7 ...  -5 -16   7]]\n",
            "[ 105  111  127 -127  127  127  127 -127 -127  127 -127 -127  127 -127\n",
            "  127  127  127 -127  127  127  127  127    7  127 -127  127  127  127\n",
            " -127 -127  -79 -127 -127 -127  127  127 -127  127 -127  127  127 -127\n",
            "  127  127 -127  127  127  127 -127  127  127 -127 -127  127 -127  127\n",
            "    6  127  127  127 -127  127  127  127 -127 -127  127  127  127 -127\n",
            "  127 -127 -127 -127 -127  127 -127 -127  127  127 -127  127  127  127\n",
            "  127  127  127  127 -127  127 -127 -127  127  127 -127  127  127  127\n",
            "  127  127 -127  127 -127  127  127  127 -127 -127  127  127 -127 -127\n",
            " -127 -127  127 -127 -127  127  127 -127  127 -127  127  127  127 -127\n",
            " -127  127 -127  127 -127  127 -127 -127  127 -127  127  127  127  127\n",
            " -127  127 -127  127 -127 -127  127  127  127  127 -127 -127  -38 -127\n",
            "   81 -127  127  127 -127 -127  127  127  127  127 -127  -60 -127  127\n",
            " -127  127  127 -127  127 -127 -127  127  127  127  127 -127 -127  127\n",
            "  127 -127  127  127 -127   56  -86 -127  127  127  127 -127  127 -127\n",
            " -127 -119  127  127  127 -127  127  127 -127  127 -127  127  127 -127\n",
            "  127  127 -127  127  127  127  127   65  127  127  127 -127  127  127\n",
            " -127  127 -127  127  127  127  127 -127  127  127  127  127  127 -127\n",
            " -127  127 -127  127  127 -127 -127  127  127 -127 -127  127 -127 -127\n",
            "  127  127 -127 -127 -127  127  127  127  127  127 -127  127  127 -127\n",
            "  -65 -127 -127 -127  123 -127 -127  127 -127 -127 -113  127  127 -127\n",
            "  127  127  127  127  127 -127  127 -127  127 -127  127  127  127  127\n",
            "  127  127  127 -127 -127  127 -127 -127  127 -127 -127  127 -127 -127\n",
            " -127  127  127  127  127  127 -127  127  127  127  127  127 -127  127\n",
            " -127  127  -58  127 -127  127  127  127  127 -127 -127  127 -127  127\n",
            "  127  127  127  127  127  127  127 -127  127 -127  127  127  127 -127\n",
            "  127  127  127  127 -127  127  127  127 -127  127 -127 -127  127 -127\n",
            "  127 -127 -127  127  127 -127  127  127  127  127 -127  127 -127 -127\n",
            " -127  127 -127 -127  127    4  127  127  127  127  127  127  127  127\n",
            " -127 -127  127  127  127  127  127  127  -60 -127 -127 -127 -127  127\n",
            "  127  127 -127 -127 -127   -2 -127  127 -127 -127  127  127  127  127\n",
            "  127 -127  127  127  127 -127  127  127 -127  127 -127  127 -127  -52\n",
            " -127  127  127  127  127  127  127 -127 -127  127 -127  127 -127 -127\n",
            "  127 -127  127   87   67  127 -127 -127 -127 -127  127 -127  127  127\n",
            " -127 -127   43 -127 -127 -127  127  127  127  127  127  127 -127 -127\n",
            " -127  127 -127 -127 -127  127 -127 -127  127 -127  127 -127  127 -127\n",
            "  127  127 -127  -69  127  127  127 -127   91 -127]\n",
            "[[ -3  -7  -1 ...  15  -5 -55]\n",
            " [-22  17  37 ...  10  42  13]\n",
            " [-26  -2  -3 ...  30  13 -54]\n",
            " ...\n",
            " [ -6  28  35 ... -27  17 -43]\n",
            " [ 17   0 -12 ...   7 -29 -34]\n",
            " [ -7  -8 -53 ...  18   0  65]]\n",
            "[-127  127 -127 -127   97  127  -25  112 -127  -20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UABDY-QAya_t"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}