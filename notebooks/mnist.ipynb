{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Imports\n"
      ],
      "metadata": {
        "id": "LGaJxAq9jEOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lpT-tFRzCMvi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707cd5cf-fd94-4412-89a6-603c329200ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e85187bb950>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from copy import deepcopy\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the neural network, load data, train it"
      ],
      "metadata": {
        "id": "AeAuE1TzjAtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set the device to use for computation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set up the network and optimizer\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Load the training data\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                #       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])\n",
        "    ),\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(10):  # 10 epochs\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = nn.CrossEntropyLoss()(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yoe-IsTCQW7",
        "outputId": "74f569c0-c95f-4155-b76d-55eabc0d734c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.305925\n",
            "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.763578\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.358299\n",
            "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.339006\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.307977\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.307055\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.416261\n",
            "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.290539\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.173276\n",
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.308258\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.322455\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.230289\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.308662\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.312016\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.149674\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.197216\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.182512\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.360825\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.152675\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.116809\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.172685\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.147442\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.065834\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.179185\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.129309\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.160169\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.169994\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.201553\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.119437\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.145129\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.158931\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.052205\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.184734\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.092108\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.055889\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.071725\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.169676\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.078485\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.204323\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.024554\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.114632\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.140726\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.034848\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.149469\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.102977\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.056241\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.061885\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.051227\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.139481\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.054017\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.112805\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.021717\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.048922\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.096806\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.036381\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.058742\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.171356\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.109858\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.158656\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.140405\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.086106\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.050062\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.125294\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.107477\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.020484\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.053520\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.041226\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.121618\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.016091\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.022835\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.058469\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.039672\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.016069\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.019058\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.043340\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.111530\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.070717\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.092738\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.130241\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.046720\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.043664\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.034956\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.031176\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.036535\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.058737\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.034970\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.080821\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.077908\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.025718\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.082434\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.052222\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.020920\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.038822\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.054167\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.062574\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.029881\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.031587\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.040439\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.022087\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.010280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the test loader and test the model"
      ],
      "metadata": {
        "id": "LUESWKlvBW1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                   #    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])\n",
        "    ),\n",
        "    batch_size=1000, shuffle=True)\n"
      ],
      "metadata": {
        "id": "pKORePvqIQqj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()  # set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():  # disable gradient computation\n",
        "        for data, target in test_loader:\n",
        "            data = torch.round(data)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += nn.CrossEntropyLoss()(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "test(model, device, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_5cjxf_BJTb",
        "outputId": "a2307b67-284d-459b-fbe7-b60c26ca0ef7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9749/10000 (97%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct and train an Observer Model.\n",
        "\n",
        "This network records the maximum and minimum output value of each layer in the training set when loaded with the learned weights from the previous step."
      ],
      "metadata": {
        "id": "qniMb94VjMnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ObserveNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ObserveNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        self.so_1_max = torch.tensor(-float('inf'))\n",
        "        self.so_2_max = torch.tensor(-float('inf'))\n",
        "        self.so_1_min = torch.tensor(float('inf'))\n",
        "        self.so_2_min = torch.tensor(float('inf'))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 784)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        temp_max = torch.max(x)\n",
        "        temp_min = torch.min(x)\n",
        "        self.so_1_max = temp_max if temp_max > self.so_1_max else self.so_1_max\n",
        "        self.so_1_min = temp_min if temp_min < self.so_1_min else self.so_1_min\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        temp_max = torch.max(x)\n",
        "        temp_min = torch.min(x)\n",
        "        self.so_2_max = temp_max if temp_max > self.so_2_max else self.so_2_max\n",
        "        self.so_2_min = temp_min if temp_min < self.so_2_min else self.so_2_min\n",
        "\n",
        "        return x\n",
        "\n",
        "q_model_dict = deepcopy(model.state_dict())\n",
        "\n",
        "o_net = ObserveNet()\n",
        "o_net.load_state_dict(q_model_dict)\n",
        "\n",
        "test(o_net, device, train_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf9F7HCLg54g",
        "outputId": "4675dc89-35d9-4e05-fa3b-6194822d0a6b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0008, Accuracy: 59113/60000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate scale constants of output for layer1 and layer2\n",
        "\n",
        "We calculate the output scale for layer1 and layer2 using the maximum and minimum output values for each layer gathered by the Observer network."
      ],
      "metadata": {
        "id": "Q9Bgm74tjcF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "so_1 = max(o_net.so_1_max, torch.abs(o_net.so_1_min)).float().item() / 127\n",
        "so_2 = max(o_net.so_2_max, torch.abs(o_net.so_2_min)).float().item() / 127\n",
        "print(so_1)\n",
        "print(so_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZN2ZHzvhm0X",
        "outputId": "0a693a98-0cff-4422-de03-b6ad163b2d3f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05730439734271192\n",
            "0.19742249316117894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize the layer weights"
      ],
      "metadata": {
        "id": "tZ87k5mRjk1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "q_model_dict = deepcopy(model.state_dict())\n",
        "\n",
        "# Returns the maximum value of a tensor devided by `m` which is the maximum n-bit int value\n",
        "# in quantization range.\n",
        "def max_scale(x: torch.tensor, m: int):\n",
        "  return torch.max(torch.abs(x)).item()/m\n",
        "\n",
        "# Scale of fc1 and fc2 determined by maximum value of int8 (127) and maximum weight value.\n",
        "def quantize_fc(x: torch.tensor, m: int):\n",
        "  return max_scale(x,m), torch.round(x / max_scale(x, m)).to(dtype=torch.int32)\n",
        "\n",
        "# Scale of bias determined by scale of the output of fc layer\n",
        "# Which is the scale of input multiplied by scale of the fc layer.\n",
        "def quantize_bias(x: torch.tensor, s: float):\n",
        "  # saturate\n",
        "  return torch.clip(torch.round(x/s), min=-127, max=127)\n",
        "\n",
        "s_fc1, q_model_dict['fc1.weight'] = quantize_fc(q_model_dict['fc1.weight'], 127)\n",
        "q_model_dict['fc1.bias'] = quantize_bias(q_model_dict['fc1.bias'], s_fc1 * 1 / 127)\n",
        "s_fc2, q_model_dict['fc2.weight'] = quantize_fc(q_model_dict['fc2.weight'], 127)\n",
        "q_model_dict['fc2.bias'] = quantize_bias(q_model_dict['fc2.bias'], s_fc2*so_1)\n",
        "\n",
        "class QuantNet(nn.Module):\n",
        "    def __init__(self, s_fc1, s_fc2, so_1, so_2):\n",
        "        super(QuantNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        self.s_fc1 = s_fc1\n",
        "        self.s_fc2 = s_fc2\n",
        "        self.so_1 = so_1\n",
        "        self.so_2 = so_2\n",
        "        self.s_x = 1 / 127\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "\n",
        "        # Scale input\n",
        "        x = torch.round(x / self.s_x)\n",
        "\n",
        "        # First layer\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        # Requantize and saturation cast\n",
        "        x = np.clip(torch.round(x * ((self.s_fc1 * self.s_x) / self.so_1)), -127, 127)\n",
        "\n",
        "        # Second layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Requantize and saturation cast\n",
        "        x = np.clip(torch.round(x * ((self.s_fc2 * self.so_1) / self.so_2)), -127, 127)\n",
        "\n",
        "        return x * self.so_2\n",
        "\n",
        "q_net = QuantNet(s_fc1=s_fc1, s_fc2=s_fc2, so_1=so_1, so_2=so_2)\n",
        "q_net.load_state_dict(q_model_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsC8AvHLldko",
        "outputId": "97219e6e-16c2-4318-9f4a-ce20001e9485"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print the scaling values so they can be used inside of Urbit"
      ],
      "metadata": {
        "id": "0GLzj2kilCSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(so_1)\n",
        "print(so_2)\n",
        "print(s_fc1)\n",
        "print(s_fc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyzedMfi9u02",
        "outputId": "06a22327-4579-41e0-adde-f9c3e65dd60c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05730439734271192\n",
            "0.19742249316117894\n",
            "0.0019561668315271692\n",
            "0.0059992356563177635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the QuantNet on the test set"
      ],
      "metadata": {
        "id": "s1fBABBmktOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the test function\n",
        "test(q_net, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S0Q85t0C266",
        "outputId": "e3dfb200-4d95-48c8-a1da-91b347264c19"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9751/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write the QuantNet weights to disk as int32"
      ],
      "metadata": {
        "id": "mSS5OZSDkwyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in q_net.named_parameters():\n",
        "    print(param.detach().int().numpy())\n",
        "    def to_byte_array(array, name):\n",
        "        # Ensure the array is float32\n",
        "        array = array.astype(np.int32)\n",
        "\n",
        "        # Flatten the array in column-major order\n",
        "        flattened = array.flatten(order='C')\n",
        "\n",
        "        # Convert to byte array\n",
        "        byte_array = flattened.tobytes()\n",
        "\n",
        "        # Write byte array to a file\n",
        "        with open(name, 'wb') as f:\n",
        "          f.write(byte_array)\n",
        "\n",
        "\n",
        "        return byte_array\n",
        "\n",
        "    # Test the function\n",
        "    to_byte_array(param.detach().int().numpy(), f'{name}.mnist')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em2AFkF2FU1m",
        "outputId": "b14bf4f9-0cae-4b31-9ad9-14ecf80e4709"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  9  -8  -4 ...   3  -2   1]\n",
            " [-14 -11 -16 ...  -1   1  -9]\n",
            " [ 14  -5 -17 ...  -7   3 -17]\n",
            " ...\n",
            " [ 16 -10 -16 ...   7 -14   9]\n",
            " [  8 -11 -14 ...  -9 -17  13]\n",
            " [ 13 -17 -12 ...  18   4  11]]\n",
            "[ 127 -127  127 -127  127  127  127  127  127 -127 -127  127  127  127\n",
            " -127  127  127 -127  127  127  127  127  127 -127  127  127 -127 -127\n",
            "  127 -127  127 -127 -127 -127 -127  127 -127  127  127  127  127 -127\n",
            " -127  127 -127 -127  127 -127 -104  127  127  127 -127  127 -127  127\n",
            " -127  127 -127 -127  127  127  127  127 -127  127 -127  127  127  127\n",
            "  127  127 -127  127  127  127 -127  127  127  127 -127  127  127 -127\n",
            "  127  127  127  127 -127 -127 -127 -127 -127  127   56 -127  127  127\n",
            "  127  127  127 -127  127  127 -127  127  127 -127  127  127  127 -127\n",
            "  127 -127  127 -127  127  127  127 -127 -127  127 -127  127  -42  127\n",
            "  127  127 -127  127  127  127 -127  127  127  127  127  127  127  127\n",
            "  127  127  127  127  127  127  127 -127 -127  127  127  127 -127 -127\n",
            "  127  127  127 -127  127  127 -127  127  127 -127 -127  127 -127  127\n",
            " -127  127 -127  127  127   63 -127 -127 -127  127  127  127  127  127\n",
            "  127  127  127 -127  127  127  127 -127  127  127  127 -127  127  127\n",
            "  127 -127  127  127 -127  127  127  127  127 -127  127  127  127 -127\n",
            " -127  127 -127  127 -127  127  127 -127  127  127  127  127 -127  127\n",
            "  -32  127 -127 -127 -127 -127 -127  127 -127  127  127  127  127  127\n",
            "  127  127 -127 -127  127 -127  127  127 -127  127  127 -127  127 -127\n",
            "  127 -127  127  127  127  127  127  127 -127 -127  127  127  127 -127\n",
            "  127 -127  127 -127  127  127 -127 -127 -127  127  127 -127  127 -127\n",
            "  127 -127 -127  127 -127  127  127  127  127 -127  127 -127  127  127\n",
            "  127  127  127  127  127 -127 -127 -127  109  127 -127  127 -127  127\n",
            "  127 -127  127 -127  -84  127 -127  127 -127 -127  127  127  127  127\n",
            "  127  127 -127 -127 -127 -127  127 -127 -127 -127  127 -127  127  127\n",
            " -127  127  127  127 -127  127  127  127  -74  127  127  127 -127  127\n",
            " -127 -127  127  127 -127 -127 -127  127 -127  127 -127   18  127  127\n",
            "  127 -127  127 -127 -127  127  127  127  127  127 -127  127  127  127\n",
            "  127  127 -127  127 -127  127  127 -127 -127 -127  127  127 -127  127\n",
            " -127  127  127  127 -127  127  127 -127  127  127  127 -127  127 -127\n",
            " -127  127 -127  127  127  127 -127  127  127  127  127  127  127 -127\n",
            " -127 -127  127  127  -56 -127  127  127  127  127 -127 -127  127  127\n",
            "  127  127 -127 -127 -127 -127  127  127  127  127 -127  127   63  127\n",
            "  127 -127  127  127 -127  127  127 -127  127  127 -127  127  127  127\n",
            "  127  127  127  127 -127  127 -127  127  127 -127 -127  127 -127   81\n",
            "  127 -127  127  127 -127 -127  127 -127  127 -127  127  127 -127  127\n",
            "  127 -127  127 -127  127 -127 -127  127  127  127]\n",
            "[[ 36 -20 -14 ... -29  10  17]\n",
            " [-23  65  10 ...  17 -25  25]\n",
            " [-26  -9 -24 ... -10 -10  17]\n",
            " ...\n",
            " [ 44   6 -12 ...  -5   1  24]\n",
            " [-33 -24 -31 ...   0 -20 -37]\n",
            " [-11 -31 -22 ...   5  -8  -1]]\n",
            "[-127  127 -127 -127  127  127  -69  127 -127  -72]\n"
          ]
        }
      ]
    }
  ]
}